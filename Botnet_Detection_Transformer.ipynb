{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnBen3wJWNXT",
        "outputId": "aca4cb8b-9fbc-4a31-b7ea-9a3af516d9f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done!\n"
          ]
        }
      ],
      "source": [
        "!pip install -q imbalanced-learn\n",
        "print('Done!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ovna6yGVWNXU",
        "outputId": "b82dfbf2-0c01-4926-9821-a7b2e410fbf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All libraries imported!\n",
            "Using device: cuda\n",
            "GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('All libraries imported!')\n",
        "print(f'Using device: {device}')\n",
        "if torch.cuda.is_available():\n",
        "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
        "else:\n",
        "    print('No GPU found - using CPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xctqDfbQWNXU"
      },
      "source": [
        "### Step 2: Uploading Dataset\n",
        "upload: `Tuesday-WorkingHours.pcap_ISCX.csv`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "PQp-jqokWNXV",
        "outputId": "725867b3-37bc-4f28-bdac-2fea1cb1cd17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Click Choose Files to upload your CSV...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6369d6af-c723-4883-af73-38a5d804f177\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6369d6af-c723-4883-af73-38a5d804f177\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2617174971.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Click Choose Files to upload your CSV...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muploaded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Uploaded: {filename}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "print('Click Choose Files to upload your CSV...')\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "print(f'Uploaded: {filename}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "multiple CSV files"
      ],
      "metadata": {
        "id": "LJ1U33uleWGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "print('Upload ALL your CSV files one by one...')\n",
        "print('(Mirai-greeth, Mirai-greip, Mirai-udpplain, Benign_Final)')\n",
        "\n",
        "# This lets you select MULTIPLE files at once\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Load and combine all uploaded files\n",
        "dfs = []\n",
        "for filename, content in uploaded.items():\n",
        "    import io\n",
        "    df_temp = pd.read_csv(io.BytesIO(content))\n",
        "    print(f'Loaded: {filename} → {df_temp.shape}')\n",
        "    dfs.append(df_temp)\n",
        "\n",
        "# Combine everything into one big dataframe\n",
        "df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# Identify the label column dynamically, making it case-insensitive\n",
        "label_col = None\n",
        "for col in df.columns:\n",
        "    if 'label' in col.lower():\n",
        "        label_col = col\n",
        "        break\n",
        "\n",
        "print(f'\\n✅ All files combined!')\n",
        "print(f'Total rows: {len(df):,}')\n",
        "print(f'Total columns: {len(df.columns)}')\n",
        "print(f'\\nLabel counts:')\n",
        "if label_col:\n",
        "    print(df[label_col].value_counts())\n",
        "else:\n",
        "    print(\"Warning: No 'label' column found (case-insensitive).\")\n",
        "    print(\"Available columns:\", df.columns.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uRr4e523eSNA",
        "outputId": "3c7b2579-3625-4a3c-885f-a10be1009a28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload ALL your CSV files one by one...\n",
            "(Mirai-greeth, Mirai-greip, Mirai-udpplain, Benign_Final)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-52c97cb9-b819-4ab5-9420-97a9eb8e5a99\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-52c97cb9-b819-4ab5-9420-97a9eb8e5a99\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Mirai-greip_flood.pcap.csv to Mirai-greip_flood.pcap (1).csv\n",
            "Saving Mirai-greip_flood1.pcap.csv to Mirai-greip_flood1.pcap (1).csv\n",
            "Saving Mirai-greip_flood2.pcap.csv to Mirai-greip_flood2.pcap (1).csv\n",
            "Saving Mirai-greip_flood3.pcap.csv to Mirai-greip_flood3.pcap (1).csv\n",
            "Saving Mirai-greip_flood4.pcap.csv to Mirai-greip_flood4.pcap (1).csv\n",
            "Saving Mirai-greip_flood5.pcap.csv to Mirai-greip_flood5.pcap (1).csv\n",
            "Saving Mirai-greip_flood6.pcap.csv to Mirai-greip_flood6.pcap (1).csv\n",
            "Saving Mirai-greip_flood7.pcap.csv to Mirai-greip_flood7.pcap (1).csv\n",
            "Saving Mirai-greip_flood8.pcap.csv to Mirai-greip_flood8.pcap (1).csv\n",
            "Saving Mirai-greip_flood9.pcap.csv to Mirai-greip_flood9.pcap (1).csv\n",
            "Saving Mirai-greip_flood10.pcap.csv to Mirai-greip_flood10.pcap (1).csv\n",
            "Saving Mirai-greip_flood11.pcap.csv to Mirai-greip_flood11.pcap (1).csv\n",
            "Saving Mirai-greip_flood12.pcap (1).csv to Mirai-greip_flood12.pcap (1) (1).csv\n",
            "Saving Mirai-greip_flood12.pcap.csv to Mirai-greip_flood12.pcap (2).csv\n",
            "Saving Mirai-greip_flood13.pcap.csv to Mirai-greip_flood13.pcap (1).csv\n",
            "Saving Mirai-greip_flood14.pcap (1).csv to Mirai-greip_flood14.pcap (1) (1).csv\n",
            "Saving Mirai-greip_flood14.pcap.csv to Mirai-greip_flood14.pcap (2).csv\n",
            "Saving Mirai-greip_flood15.pcap.csv to Mirai-greip_flood15.pcap (1).csv\n",
            "Saving Mirai-greip_flood16.pcap (1).csv to Mirai-greip_flood16.pcap (1) (1).csv\n",
            "Saving Mirai-greip_flood16.pcap.csv to Mirai-greip_flood16.pcap (2).csv\n",
            "Saving Mirai-greip_flood17.pcap.csv to Mirai-greip_flood17.pcap (1).csv\n",
            "Saving Mirai-greip_flood18.pcap.csv to Mirai-greip_flood18.pcap (1).csv\n",
            "Saving Mirai-greip_flood19.pcap (1).csv to Mirai-greip_flood19.pcap (1) (1).csv\n",
            "Saving Mirai-greip_flood19.pcap.csv to Mirai-greip_flood19.pcap (2).csv\n",
            "Saving Mirai-greip_flood20.pcap.csv to Mirai-greip_flood20.pcap (1).csv\n",
            "Saving Mirai-greip_flood21.pcap.csv to Mirai-greip_flood21.pcap (1).csv\n",
            "Loaded: Mirai-greip_flood.pcap (1).csv → (35073, 39)\n",
            "Loaded: Mirai-greip_flood1.pcap (1).csv → (35128, 39)\n",
            "Loaded: Mirai-greip_flood2.pcap (1).csv → (34846, 39)\n",
            "Loaded: Mirai-greip_flood3.pcap (1).csv → (35168, 39)\n",
            "Loaded: Mirai-greip_flood4.pcap (1).csv → (35784, 39)\n",
            "Loaded: Mirai-greip_flood5.pcap (1).csv → (35456, 39)\n",
            "Loaded: Mirai-greip_flood6.pcap (1).csv → (36052, 39)\n",
            "Loaded: Mirai-greip_flood7.pcap (1).csv → (35462, 39)\n",
            "Loaded: Mirai-greip_flood8.pcap (1).csv → (35755, 39)\n",
            "Loaded: Mirai-greip_flood9.pcap (1).csv → (35044, 39)\n",
            "Loaded: Mirai-greip_flood10.pcap (1).csv → (35292, 39)\n",
            "Loaded: Mirai-greip_flood11.pcap (1).csv → (35019, 39)\n",
            "Loaded: Mirai-greip_flood12.pcap (1) (1).csv → (35339, 39)\n",
            "Loaded: Mirai-greip_flood12.pcap (2).csv → (35339, 39)\n",
            "Loaded: Mirai-greip_flood13.pcap (1).csv → (35251, 39)\n",
            "Loaded: Mirai-greip_flood14.pcap (1) (1).csv → (35098, 39)\n",
            "Loaded: Mirai-greip_flood14.pcap (2).csv → (35098, 39)\n",
            "Loaded: Mirai-greip_flood15.pcap (1).csv → (35105, 39)\n",
            "Loaded: Mirai-greip_flood16.pcap (1) (1).csv → (34920, 39)\n",
            "Loaded: Mirai-greip_flood16.pcap (2).csv → (34920, 39)\n",
            "Loaded: Mirai-greip_flood17.pcap (1).csv → (35948, 39)\n",
            "Loaded: Mirai-greip_flood18.pcap (1).csv → (33982, 39)\n",
            "Loaded: Mirai-greip_flood19.pcap (1) (1).csv → (34844, 39)\n",
            "Loaded: Mirai-greip_flood19.pcap (2).csv → (34844, 39)\n",
            "Loaded: Mirai-greip_flood20.pcap (1).csv → (35089, 39)\n",
            "Loaded: Mirai-greip_flood21.pcap (1).csv → (11991, 39)\n",
            "\n",
            "✅ All files combined!\n",
            "Total rows: 891,847\n",
            "Total columns: 39\n",
            "\n",
            "Label counts:\n",
            "Warning: No 'label' column found (case-insensitive).\n",
            "Available columns: ['Header_Length', 'Protocol Type', 'Time_To_Live', 'Rate', 'fin_flag_number', 'syn_flag_number', 'rst_flag_number', 'psh_flag_number', 'ack_flag_number', 'ece_flag_number', 'cwr_flag_number', 'ack_count', 'syn_count', 'fin_count', 'rst_count', 'HTTP', 'HTTPS', 'DNS', 'Telnet', 'SMTP', 'SSH', 'IRC', 'TCP', 'UDP', 'DHCP', 'ARP', 'ICMP', 'IGMP', 'IPv', 'LLC', 'Tot sum', 'Min', 'Max', 'AVG', 'Std', 'Tot size', 'IAT', 'Number', 'Variance']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('best_model.pth')  # Save your trained model!"
      ],
      "metadata": {
        "id": "ta6GlpcJnBT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing\n"
      ],
      "metadata": {
        "id": "MujUcoscmsQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "# Check what files were uploaded\n",
        "print(\"Files uploaded:\")\n",
        "for filename in uploaded.keys():\n",
        "    print(f\"  → {filename}\")"
      ],
      "metadata": {
        "id": "8aYkCM3mmtvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(\"ALL FILES IN COLAB RIGHT NOW:\")\n",
        "for f in os.listdir('/content'):\n",
        "    size = os.path.getsize(f'/content/{f}') / (1024*1024)\n",
        "    print(f\"  → {f}  ({size:.1f} MB)\")"
      ],
      "metadata": {
        "id": "NsxIkbgznFIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "collab\n"
      ],
      "metadata": {
        "id": "AmZz_1Msnasu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "print(\"Loading all files...\")\n",
        "\n",
        "# Load all Mirai (botnet) files\n",
        "mirai_files = glob.glob('/content/Mirai-*.csv')\n",
        "benign_files = glob.glob('/content/BenignTraffic*.csv')\n",
        "\n",
        "print(f\"Found {len(mirai_files)} Mirai files\")\n",
        "print(f\"Found {len(benign_files)} Benign files\")\n",
        "\n",
        "# Load Mirai files and label them as 1 (attack)\n",
        "mirai_dfs = []\n",
        "for f in mirai_files:\n",
        "    df_temp = pd.read_csv(f)\n",
        "    df_temp['label'] = 1  # 1 = Botnet attack\n",
        "    mirai_dfs.append(df_temp)\n",
        "\n",
        "# Load Benign files and label them as 0 (normal)\n",
        "benign_dfs = []\n",
        "for f in benign_files:\n",
        "    df_temp = pd.read_csv(f)\n",
        "    df_temp['label'] = 0  # 0 = Normal traffic\n",
        "    benign_dfs.append(df_temp)\n",
        "\n",
        "# Combine everything\n",
        "df = pd.concat(mirai_dfs + benign_dfs, ignore_index=True)\n",
        "\n",
        "print(f\"\\n✅ DATASET READY!\")\n",
        "print(f\"Total rows    : {len(df):,}\")\n",
        "print(f\"Total columns : {len(df.columns)}\")\n",
        "print(f\"\\nClass distribution:\")\n",
        "print(f\"  Botnet (1) : {(df['label']==1).sum():,}\")\n",
        "print(f\"  Benign (0) : {(df['label']==0).sum():,}\")"
      ],
      "metadata": {
        "id": "Ulhz-RpUncZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGwGPII8WNXV"
      },
      "source": [
        "### Step 3: Load and Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4V_2ysxpWNXW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# ── STEP 1: Load ALL files ──────────────────\n",
        "print(\"\\nLoading all files...\")\n",
        "\n",
        "mirai_files  = glob.glob('/content/Mirai-*.csv')\n",
        "benign_files = glob.glob('/content/BenignTraffic*.csv')\n",
        "print(f'Mirai files  : {len(mirai_files)}')\n",
        "print(f'Benign files : {len(benign_files)}')\n",
        "\n",
        "# Load Mirai → label 1 (attack)\n",
        "mirai_dfs = []\n",
        "for f in mirai_files:\n",
        "    df_temp = pd.read_csv(f)\n",
        "    df_temp['label'] = 1\n",
        "    mirai_dfs.append(df_temp)\n",
        "\n",
        "# Load Benign → label 0 (normal)\n",
        "benign_dfs = []\n",
        "for f in benign_files:\n",
        "    df_temp = pd.read_csv(f)\n",
        "    df_temp['label'] = 0\n",
        "    benign_dfs.append(df_temp)\n",
        "\n",
        "# Combine all into one dataframe\n",
        "df = pd.concat(mirai_dfs + benign_dfs, ignore_index=True)\n",
        "print(f'\\nTotal rows    : {len(df):,}')\n",
        "print(f'Total columns : {len(df.columns)}')\n",
        "\n",
        "# ── STEP 2: Clean Data ──────────────────────\n",
        "print(\"\\nCleaning data...\")\n",
        "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "df.fillna(0, inplace=True)\n",
        "\n",
        "# ── STEP 3: Features and Labels ─────────────\n",
        "y_binary  = df['label']\n",
        "X_numeric = df.drop(['label'], axis=1).select_dtypes(include=[np.number])\n",
        "\n",
        "print(f'\\nFeatures : {X_numeric.shape[1]}')\n",
        "print(f'Benign   : {(y_binary==0).sum():,} ({(y_binary==0).mean()*100:.1f}%)')\n",
        "print(f'Botnet   : {(y_binary==1).sum():,} ({(y_binary==1).mean()*100:.1f}%)')\n",
        "\n",
        "# ── STEP 4: Plot Class Distribution ─────────\n",
        "plt.figure(figsize=(8, 4))\n",
        "y_binary.value_counts().plot(kind='bar', color=['steelblue','tomato'])\n",
        "plt.title('Class Distribution')\n",
        "plt.xlabel('0 = Benign, 1 = Botnet')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ── STEP 5: Sample ───────────────────────────\n",
        "SAMPLE_SIZE     = 100000\n",
        "SEQUENCE_LENGTH = 10\n",
        "\n",
        "print(f'\\nSampling {SAMPLE_SIZE:,} rows...')\n",
        "idx       = np.random.choice(len(X_numeric), SAMPLE_SIZE, replace=False)\n",
        "X_sampled = X_numeric.iloc[idx].reset_index(drop=True)\n",
        "y_sampled = y_binary.iloc[idx].reset_index(drop=True)\n",
        "\n",
        "# ── STEP 6: Scale ────────────────────────────\n",
        "scaler   = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_sampled)\n",
        "\n",
        "# ── STEP 7: Create Sequences ─────────────────\n",
        "print(f'Creating sequences of {SEQUENCE_LENGTH} packets...')\n",
        "X_seq, y_seq = [], []\n",
        "for i in range(len(X_scaled) - SEQUENCE_LENGTH + 1):\n",
        "    X_seq.append(X_scaled[i:i+SEQUENCE_LENGTH])\n",
        "    y_seq.append(y_sampled.values[i+SEQUENCE_LENGTH-1])\n",
        "\n",
        "X_seq = np.array(X_seq, dtype=np.float32)\n",
        "y_seq = np.array(y_seq, dtype=np.float32)\n",
        "\n",
        "# ── STEP 8: Train/Test Split ─────────────────\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_seq, y_seq, test_size=0.2, random_state=42, stratify=y_seq\n",
        ")\n",
        "\n",
        "print(f'\\nTrain sequences : {len(X_train):,}')\n",
        "print(f'Test  sequences : {len(X_test):,}')\n",
        "print(f'Sequence shape  : {X_train.shape[1:]}')\n",
        "print('\\n✅ Data ready! Next step: Build the Transformer model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nc2ybLf_WNXW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rCMc5gXWNXW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juK11SljWNXX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDECmeKLWNXX"
      },
      "source": [
        "### Step 4: Build Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jA1yln3SWNXX"
      },
      "outputs": [],
      "source": [
        "# ── IMPROVED MODEL ───────────────────────────\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe       = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "\n",
        "class BotnetTransformer(nn.Module):\n",
        "    def __init__(self, input_dim, d_model=256, nhead=8, num_layers=3, dropout=0.2):\n",
        "        super().__init__()\n",
        "\n",
        "        # IMPROVED: Better input projection (was just one Linear layer)\n",
        "        self.input_projection = nn.Sequential(\n",
        "            nn.Linear(input_dim, d_model),\n",
        "            nn.LayerNorm(d_model),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "\n",
        "        # IMPROVED: Bigger model (was d_model=128, nhead=4, layers=2)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, nhead=nhead,\n",
        "            dim_feedforward=d_model*4,\n",
        "            dropout=dropout, batch_first=True,\n",
        "            norm_first=True              # More stable training\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            encoder_layer, num_layers=num_layers,\n",
        "            norm=nn.LayerNorm(d_model)\n",
        "        )\n",
        "\n",
        "        # IMPROVED: Better classifier head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(d_model, 128),\n",
        "            nn.LayerNorm(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout/2),\n",
        "            nn.Linear(64, 1)\n",
        "            # NO Sigmoid here — BCEWithLogitsLoss handles it\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_projection(x)\n",
        "        x = self.pos_encoder(x)\n",
        "        x = self.transformer(x)\n",
        "        x = x.mean(dim=1)              # IMPROVED: Mean pooling (was last token only)\n",
        "        return self.classifier(x).squeeze()\n",
        "\n",
        "\n",
        "input_dim = X_train.shape[2]\n",
        "model     = BotnetTransformer(input_dim=input_dim).to(device)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "print(f'✅ Model ready!')\n",
        "print(f'Parameters : {total_params:,}')\n",
        "print(f'Device     : {device}')\n",
        "print(f'')\n",
        "print(f'Improvements over your original:')\n",
        "print(f'  OLD: d_model=128, heads=4, layers=2, last-token pooling')\n",
        "print(f'  NEW: d_model=256, heads=8, layers=3, mean pooling')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjVC2e2GWNXX"
      },
      "source": [
        "### Step 5: Train the Model (Takes 5-15 mins)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EflixJCOWNXX"
      },
      "outputs": [],
      "source": [
        "# ── TRAINING SETUP ───────────────────────────\n",
        "EPOCHS      = 30\n",
        "BATCH_SIZE  = 128    # was 64 — bigger = faster on GPU\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# Handle class imbalance — OLD CODE MISSING THIS!\n",
        "neg_count  = (y_train == 0).sum()\n",
        "pos_count  = (y_train == 1).sum()\n",
        "pos_weight = torch.tensor([neg_count / pos_count]).to(device)\n",
        "print(f'Class weight : {pos_weight.item():.2f}x')\n",
        "\n",
        "# Prepare tensors\n",
        "X_train_t = torch.FloatTensor(X_train).to(device)\n",
        "y_train_t = torch.FloatTensor(y_train).to(device)\n",
        "X_test_t  = torch.FloatTensor(X_test).to(device)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    TensorDataset(X_train_t, y_train_t),\n",
        "    batch_size=BATCH_SIZE, shuffle=True\n",
        ")\n",
        "\n",
        "# OLD: nn.BCELoss()  ← crashes because model has no Sigmoid now\n",
        "# NEW: BCEWithLogitsLoss ← handles imbalance + more stable\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
        "\n",
        "# OLD: StepLR — reduces every 5 epochs blindly\n",
        "# NEW: ReduceLROnPlateau — reduces only when model stops improving\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='max', patience=3, factor=0.5\n",
        ")\n",
        "\n",
        "# ── EARLY STOPPING ── OLD CODE MISSING THIS!\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7):\n",
        "        self.patience    = patience\n",
        "        self.counter     = 0\n",
        "        self.best_score  = None\n",
        "        self.should_stop = False\n",
        "\n",
        "    def __call__(self, score):\n",
        "        if self.best_score is None or score > self.best_score + 0.001:\n",
        "            self.best_score = score\n",
        "            self.counter    = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.should_stop = True\n",
        "\n",
        "early_stopping = EarlyStopping(patience=7)\n",
        "\n",
        "# ── TRAIN ────────────────────────────────────\n",
        "train_losses = []\n",
        "test_f1s     = []\n",
        "best_f1      = 0\n",
        "\n",
        "print('Training started...')\n",
        "print('-' * 65)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # Training\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(model(batch_X), batch_y)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_loss = epoch_loss / len(train_loader)\n",
        "    train_losses.append(avg_loss)\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(X_test_t).cpu().numpy()\n",
        "        probs  = torch.sigmoid(torch.tensor(logits)).numpy()\n",
        "        preds  = (probs > 0.5).astype(int)\n",
        "\n",
        "    acc = accuracy_score(y_test, preds)\n",
        "    f1  = f1_score(y_test, preds, zero_division=0)\n",
        "    test_f1s.append(f1)\n",
        "\n",
        "    tag = ''\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "        tag = ' ← BEST'\n",
        "\n",
        "    scheduler.step(f1)\n",
        "    lr = optimizer.param_groups[0]['lr']\n",
        "    print(f'Epoch {epoch+1:2d}/{EPOCHS} | Loss: {avg_loss:.4f} | Acc: {acc:.4f} | F1: {f1:.4f} | LR: {lr:.6f}{tag}')\n",
        "\n",
        "    early_stopping(f1)\n",
        "    if early_stopping.should_stop:\n",
        "        print(f'\\nEarly stopping at epoch {epoch+1}')\n",
        "        break\n",
        "\n",
        "print('-' * 65)\n",
        "print(f'✅ Training complete! Best F1: {best_f1:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6pLJZhgWNXX"
      },
      "source": [
        "### Step 6: Results and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkZ_xNf6WNXY"
      },
      "outputs": [],
      "source": [
        "# ── FINAL EVALUATION & CHARTS ────────────────\n",
        "from sklearn.metrics import roc_curve, precision_recall_curve, average_precision_score, precision_recall_fscore_support\n",
        "\n",
        "# Load best model\n",
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = model(X_test_t).cpu().numpy()\n",
        "    probs  = torch.sigmoid(torch.tensor(logits)).numpy()\n",
        "    preds  = (probs > 0.5).astype(int)\n",
        "\n",
        "# Metrics\n",
        "accuracy           = accuracy_score(y_test, preds)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(y_test, preds, average='binary')\n",
        "auc                = roc_auc_score(y_test, probs)\n",
        "\n",
        "print('=' * 50)\n",
        "print('       YOUR FINAL RESULTS')\n",
        "print('=' * 50)\n",
        "print(f'  Accuracy  : {accuracy:.4f} ({accuracy*100:.2f}%)')\n",
        "print(f'  Precision : {precision:.4f}')\n",
        "print(f'  Recall    : {recall:.4f}')\n",
        "print(f'  F1-Score  : {f1:.4f}')\n",
        "print(f'  ROC-AUC   : {auc:.4f}')\n",
        "print('=' * 50)\n",
        "print(classification_report(y_test, preds, target_names=['Benign','Botnet']))\n",
        "\n",
        "# ── CHART 1: Training Curves ─────────────────\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "fig.suptitle('Botnet Detection - Transformer Results', fontsize=14, fontweight='bold')\n",
        "\n",
        "axes[0].plot(range(1, len(train_losses)+1), train_losses, 'b-o', linewidth=2)\n",
        "axes[0].set_title('Training Loss')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].plot(range(1, len(test_f1s)+1), test_f1s, 'g-o', linewidth=2)\n",
        "axes[1].set_title('F1 Score Per Epoch')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('F1 Score')\n",
        "axes[1].set_ylim([0, 1.05])\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_curves.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# ── CHART 2: Confusion Matrix ─────────────────\n",
        "plt.figure(figsize=(7, 5))\n",
        "cm = confusion_matrix(y_test, preds)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Benign','Botnet'],\n",
        "            yticklabels=['Benign','Botnet'])\n",
        "plt.title('Confusion Matrix', fontsize=13, fontweight='bold')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.tight_layout()\n",
        "plt.savefig('confusion_matrix.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# ── CHART 3: ROC Curve ───────────────────────\n",
        "plt.figure(figsize=(7, 5))\n",
        "fpr, tpr, _ = roc_curve(y_test, probs)\n",
        "plt.plot(fpr, tpr, 'b-', linewidth=2, label=f'Transformer (AUC={auc:.4f})')\n",
        "plt.plot([0,1],[0,1],'k--', alpha=0.5, label='Random')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve', fontsize=13, fontweight='bold')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('roc_curve.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# ── CHART 4: Metrics Bar Chart ────────────────\n",
        "plt.figure(figsize=(10, 5))\n",
        "metrics = {'Accuracy': accuracy, 'Precision': precision,\n",
        "           'Recall': recall, 'F1-Score': f1, 'ROC-AUC': auc}\n",
        "colors = ['#2196F3','#4CAF50','#FF9800','#E91E63','#9C27B0']\n",
        "bars = plt.bar(metrics.keys(), metrics.values(), color=colors, edgecolor='black')\n",
        "for bar, val in zip(bars, metrics.values()):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2,\n",
        "             bar.get_height() + 0.01,\n",
        "             f'{val:.4f}', ha='center', fontweight='bold', fontsize=11)\n",
        "plt.ylim(0, 1.15)\n",
        "plt.title('Model Performance Metrics', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Score')\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('metrics.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# ── DOWNLOAD ALL FILES ────────────────────────\n",
        "from google.colab import files\n",
        "files.download('best_model.pth')\n",
        "files.download('training_curves.png')\n",
        "files.download('confusion_matrix.png')\n",
        "files.download('roc_curve.png')\n",
        "files.download('metrics.png')\n",
        "print('✅ All files downloaded!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmXRtSxtWNXY"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "fig.suptitle('Botnet Detection - Transformer Results', fontsize=16, fontweight='bold')\n",
        "\n",
        "axes[0].plot(range(1, len(train_losses)+1), train_losses, 'b-o', linewidth=2)\n",
        "axes[0].set_title('Training Loss')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].plot(range(1, len(test_f1s)+1), test_f1s, 'g-o', linewidth=2)\n",
        "axes[1].set_title('Test F1 Score')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('F1 Score')\n",
        "axes[1].set_ylim([0, 1])\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "cm = confusion_matrix(y_test, preds)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Benign','Botnet'],\n",
        "            yticklabels=['Benign','Botnet'], ax=axes[2])\n",
        "axes[2].set_title('Confusion Matrix')\n",
        "axes[2].set_ylabel('True Label')\n",
        "axes[2].set_xlabel('Predicted Label')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('results.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "teEosFY8WNXY"
      },
      "outputs": [],
      "source": [
        "metrics = {'Accuracy': accuracy, 'Precision': precision,\n",
        "           'Recall': recall, 'F1-Score': f1, 'ROC-AUC': auc}\n",
        "colors = ['#2196F3','#4CAF50','#FF9800','#E91E63','#9C27B0']\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "bars = plt.bar(metrics.keys(), metrics.values(), color=colors)\n",
        "for bar, val in zip(bars, metrics.values()):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2,\n",
        "             bar.get_height() + 0.01,\n",
        "             f'{val:.4f}', ha='center', fontweight='bold', fontsize=12)\n",
        "plt.ylim(0, 1.15)\n",
        "plt.title('Model Performance Metrics', fontsize=15, fontweight='bold')\n",
        "plt.ylabel('Score')\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('metrics.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print('All charts saved!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvi61o1SWNXY"
      },
      "source": [
        "### Step 7: Download Your Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UI2VAnaJWNXY"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download('results.png')\n",
        "files.download('metrics.png')\n",
        "files.download('best_model.pth')\n",
        "print('Downloaded: results.png, metrics.png, best_model.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1S2ibMSNWNXY"
      },
      "source": [
        "## Congratulations! Project Complete!\n",
        "\n",
        "**For your report include:**\n",
        "- Dataset: CICIDS2017 - Tuesday Working Hours\n",
        "- Model: Transformer Encoder with Multi-Head Self-Attention (4 heads, 2 layers)\n",
        "- Features: 77 network flow features\n",
        "- Training: 20 epochs, Adam optimizer, Binary Cross Entropy Loss\n",
        "- Results: Your accuracy, F1-score, confusion matrix charts\n"
      ]
    }
  ]
}